{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92dab33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T02:15:29.169645Z",
     "start_time": "2023-05-20T02:15:20.157250Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#np.set_printoptions(threshold=np.inf)\n",
    "pd.set_option('display.width', 500) # 设置字符显示宽度\n",
    "pd.set_option('display.max_rows', 10) # 设置显示最大行\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "def read_csv_file(user_id, video_id):\n",
    "    filepath = f'../../../data/360_video_viewing_dataset/sensory/raw/{video_id}_user{user_id}_raw.csv'\n",
    "    #print(filepath)\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['video_id'] = video_id  # 添加video_id字段\n",
    "    return df\n",
    "\n",
    "data_with_labels = []\n",
    "video_ids = ['coaster', 'coaster2', 'diving', 'drive', 'game', 'landscape', 'pacman', 'panel', 'ride', 'sport']\n",
    "for user_id in range(1, 51):\n",
    "    user_data = []\n",
    "    for video_id in range(len(video_ids)):\n",
    "        if len(str(user_id)) == 1:\n",
    "            user_id = '0' + str(user_id)\n",
    "        df = read_csv_file(user_id, video_ids[video_id])\n",
    "\n",
    "        # 对数据进行预处理，例如计算每秒的坐标等\n",
    "        # df = preprocess(df)\n",
    " \n",
    "        # 添加标签（用户ID）\n",
    "        df['user_id'] = user_id\n",
    "\n",
    "        user_data.append(df)\n",
    "\n",
    "    user_data_combined = pd.concat(user_data, ignore_index=True)\n",
    "    data_with_labels.append(user_data_combined)\n",
    "\n",
    "data_with_labels = pd.concat(data_with_labels, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4049c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T23:37:49.271567Z",
     "start_time": "2023-05-19T23:20:17.796833Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from keras.layers import Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Bidirectional\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],), initializer='zeros', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        q = tf.nn.tanh(tf.linalg.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(tf.reduce_sum(q * x, axis=-1), axis=-1)\n",
    "        return tf.reduce_sum(a[:, :, tf.newaxis] * x, axis=1)\n",
    "\n",
    "def process_data(data, i):\n",
    "\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n",
    "    data['rounded_time'] = pd.to_datetime(data['timestamp'], unit='s').dt.floor('100ms')\n",
    "    \n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    print(\"Processing\",i)\n",
    "    for user_id, user_data in tqdm(data.groupby('user_id')):\n",
    "        for video_id, video_data in user_data.groupby('video_id'):\n",
    "            for second, second_data in video_data.groupby(video_data['rounded_time'].dt.floor('1s')):\n",
    "                time_slices = []\n",
    "                for _, chunk_data in second_data.groupby(pd.cut(pd.to_datetime(second_data['timestamp']), pd.date_range(second, second + timedelta(seconds=1), periods=11)), observed = True):\n",
    "\n",
    "                    if not chunk_data.empty:\n",
    "                        chunk_data = chunk_data[['rawTX', 'rawTY', 'rawTZ', 'rawYaw', 'rawPitch', 'rawRoll']]\n",
    "                        time_slices.append(chunk_data.mean().values)\n",
    "\n",
    "                if len(time_slices) == 10:\n",
    "                    if video_id == video_ids[i]:\n",
    "                        test_data.append(time_slices)\n",
    "                        test_labels.append(user_id)\n",
    "                    else:\n",
    "                        train_data.append(time_slices)\n",
    "                        train_labels.append(user_id)\n",
    "\n",
    "    return np.array(train_data), np.array(train_labels), np.array(test_data), np.array(test_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data_with_labels dataframe here\n",
    "# data_with_labels = pd.DataFrame(...)\n",
    "\n",
    "# Define the path to save the models\n",
    "model_path = './model_{}.tflite'\n",
    "\n",
    "for i in range(9):\n",
    "    train_data, train_labels, test_data, test_labels = process_data(data_with_labels, i)\n",
    "    print(train_data, train_labels, test_data, test_labels)\n",
    "    # Convert user IDs to integers\n",
    "    unique_labels = np.unique(np.concatenate([train_labels, test_labels]))\n",
    "    label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    int_train_labels = np.array([label_map[label] for label in train_labels])\n",
    "    int_test_labels = np.array([label_map[label] for label in test_labels])\n",
    "    \n",
    "    # Convert label data to one-hot encoding\n",
    "    one_hot_train_labels = to_categorical(int_train_labels, num_classes=len(unique_labels))\n",
    "    one_hot_test_labels = to_categorical(int_test_labels, num_classes=len(unique_labels))\n",
    "\n",
    "    # Train the model\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        # Define the model\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(128, input_shape=(10, 6), return_sequences=True)))\n",
    "        model.add(AttentionLayer())\n",
    "        model.add(Reshape((1, -1)))\n",
    "        model.add(Bidirectional(LSTM(64)))\n",
    "        model.add(Dense(len(unique_labels), activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        print(f'train_data.shape: {train_data.shape}')\n",
    "        print(f'test_data.shape: {test_data.shape}')\n",
    "        print(f'train_data.len: {len(train_data)}')\n",
    "        print(f'test_data.len: {len(test_data)}')\n",
    "\n",
    "        # Reshape the training and testing data\n",
    "        train_data = train_data.reshape(train_data.shape[0], 10, 6)\n",
    "        test_data = test_data.reshape(test_data.shape[0], 10, 6)\n",
    "\n",
    "        # Train the model\n",
    "        batch_size = min(128, train_data.shape[0])\n",
    "        print(f'batch_size: {batch_size}')\n",
    "\n",
    "        model.fit(train_data, one_hot_train_labels, epochs=50, batch_size=batch_size, validation_data=(test_data, one_hot_test_labels))\n",
    "\n",
    "        # Save the model to a file\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "        # Save the model.\n",
    "        with open(model_path.format(i), 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"Model {i} saved to {model_path.format(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b6683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T23:37:50.753527Z",
     "start_time": "2023-05-19T23:37:50.725529Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026e7de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T23:44:15.773635Z",
     "start_time": "2023-05-19T23:41:56.904019Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Reshape\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "# 将用户ID转换为从0开始的整数\n",
    "unique_labels = np.unique(np.concatenate([train_labels, test_labels]))\n",
    "label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "int_train_labels = np.array([label_map[label] for label in train_labels])\n",
    "int_test_labels = np.array([label_map[label] for label in test_labels])\n",
    "\n",
    "# 将标签数据转换为one-hot编码\n",
    "one_hot_train_labels = to_categorical(int_train_labels, num_classes=len(unique_labels))\n",
    "one_hot_test_labels = to_categorical(int_test_labels, num_classes=len(unique_labels))\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],), initializer='zeros', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        q = tf.nn.tanh(tf.linalg.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(tf.reduce_sum(q * x, axis=-1), axis=-1)\n",
    "        return tf.reduce_sum(a[:, :, tf.newaxis] * x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    # 定义模型\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(10, 6), return_sequences=True))\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(Reshape((1, -1)))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(len(unique_labels), activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 重塑训练和测试数据\n",
    "    train_data = train_data.reshape(train_data.shape[0], 10, 6)\n",
    "    test_data = test_data.reshape(test_data.shape[0], 10, 6)\n",
    "\n",
    "    # 训练模型\n",
    "    batch_size = min(128, train_data.shape[0])\n",
    "    model.fit(train_data, one_hot_train_labels, epochs=50, batch_size=batch_size, validation_data=(test_data, one_hot_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310e212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T23:00:51.988297Z",
     "start_time": "2023-05-19T23:00:50.202075Z"
    }
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "with open('modelB.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24989578",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172a975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T23:56:51.460948Z",
     "start_time": "2023-05-19T23:44:22.511471Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pandas')\n",
    "import seaborn as sns\n",
    "from keras.utils import to_categorical\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def majority_vote(predictions, window_size):\n",
    "    vote_results = []\n",
    "    for i in range(0, len(predictions) - window_size + 1, window_size):\n",
    "        votes = predictions[i:i+window_size]\n",
    "        vote_result = np.argmax(np.bincount(votes))\n",
    "        vote_results.append(vote_result)\n",
    "    return vote_results\n",
    "\n",
    "def compute_accuracy_with_window(model, data_with_labels, label_map, window_sizes):\n",
    "    acc_results = []\n",
    "\n",
    "    for video_id in range(9):\n",
    "        for window_size in window_sizes:\n",
    "            accuracy_per_window = []\n",
    "            for user_id in range(1, 51):\n",
    "                if len(str(user_id)) == 1:\n",
    "                    user_id = '0' + str(user_id)\n",
    "                # 从原始数据中筛选出user_id对应的测试集和video_id的数据\n",
    "                test_data = data_with_labels[(data_with_labels['user_id'] == user_id) & (data_with_labels['video_id'] == video_ids[video_id])]\n",
    "\n",
    "                if test_data.empty:\n",
    "                    continue\n",
    "\n",
    "                # 处理数据并获取对应的标签\n",
    "                train_data, train_labels, test_data, test_labels = process_data(test_data, video_id)\n",
    "                # print(f'test_data: {test_data}')\n",
    "                # print(f'test_labels: {test_labels}')\n",
    "                # print(f'label_map[\"label\"]: {label_map[\"10\"]}')\n",
    "                # print('------------')\n",
    "                # for label in test_labels:\n",
    "                #     print(f'label: {label}, type(label): {type(label)}')\n",
    "                #     print(label_map)\n",
    "                #     print(label_map['10'])\n",
    "                #     print(label_map[str(label)])\n",
    "                # print('------------')\n",
    "                int_test_labels = np.array([label_map[str(label)] for label in test_labels])\n",
    "                one_hot_test_labels = to_categorical(int_test_labels, num_classes=len(label_map))\n",
    "\n",
    "                # 重塑测试数据\n",
    "                test_data = test_data.reshape(test_data.shape[0], 10, 6)\n",
    "\n",
    "                # 获取模型预测\n",
    "                predictions = model.predict(test_data)\n",
    "                predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "                # 应用滑动窗口和多数投票\n",
    "                vote_results = majority_vote(predictions, window_size)\n",
    "\n",
    "                # 计算滑动窗口和多数投票后的准确率\n",
    "                correct = np.sum(vote_results == int_test_labels[:len(vote_results)])\n",
    "                accuracy = correct / len(vote_results)\n",
    "                accuracy_per_window.append(accuracy)\n",
    "\n",
    "            # 计算当前窗口大小下所有用户的平均准确率\n",
    "            acc_results.append((video_id, window_size, np.mean(accuracy_per_window)))\n",
    "            print('Video ID: %d, Window size: %d, average accuracy: %.2f' % (video_id, window_size, np.mean(accuracy_per_window)))\n",
    "\n",
    "    return acc_results\n",
    "\n",
    "# 设置窗口大小范围\n",
    "window_sizes = list(range(1, 6, 1))\n",
    "\n",
    "# 计算各个窗口大小下的准确率\n",
    "#print(f'label_map: {label_map}')\n",
    "accuracy_results = compute_accuracy_with_window(model, data_with_labels, label_map, window_sizes)\n",
    "\n",
    "# 转换成pandas DataFrame格式以便绘图\n",
    "accuracy_df = pd.DataFrame(accuracy_results, columns=['video_id', 'window_size', 'accuracy'])\n",
    "\n",
    "# 绘制折线图\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(data=accuracy_df, x='window_size', y='accuracy', hue='video_id', palette='tab10')\n",
    "plt.xlabel('Window Size (M)')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.title('Average Accuracy for each video with varying window size')\n",
    "plt.xticks(window_sizes)\n",
    "plt.grid(True)\n",
    "plt.legend(title='Video ID', loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(data=accuracy_df, x='window_size', y='accuracy')\n",
    "plt.xlabel('Window Size (M)')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.title('Average Accuracy for video 0 with varying window size')\n",
    "plt.xticks(window_sizes)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1542442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T01:22:46.443597Z",
     "start_time": "2023-05-20T01:09:30.705224Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy_per_user(model, data_with_labels, unique_labels, label_map):\n",
    "    user_acc_results = {}\n",
    "\n",
    "    for user_id in range(1, 51): # assume user_id from 1 to 50\n",
    "        acc_results = []\n",
    "        if len(str(user_id)) == 1:\n",
    "            user_id = '0' + str(user_id)\n",
    "        user_id = str(user_id)\n",
    "        for i in range(0, 9):\n",
    "            # 从原始数据中筛选出video_id和user_id对应的测试集\n",
    "            test_data = data_with_labels[(data_with_labels['video_id'] == video_ids[i]) & (data_with_labels['user_id'] == user_id)]\n",
    "            #print(test_data)\n",
    "            # 如果测试数据为空，跳过此轮循环\n",
    "            if test_data.empty:\n",
    "                continue\n",
    "\n",
    "            # 处理数据并获取对应的标签\n",
    "            train_data, train_labels, test_data, test_labels = process_data(test_data, i)\n",
    "            int_test_labels = np.array([label_map[str(label)] for label in test_labels])\n",
    "            one_hot_test_labels = to_categorical(int_test_labels, num_classes=len(unique_labels))\n",
    "\n",
    "            # 重塑测试数据\n",
    "            test_data = test_data.reshape(test_data.shape[0], 10, 6)\n",
    "\n",
    "            try:\n",
    "                # 计算准确率\n",
    "                loss, accuracy = model.evaluate(test_data, one_hot_test_labels, batch_size=len(test_data), verbose=0)\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping evaluation for user_id: {user_id}, video_id: {i}. Evaluation returned: {e}\")\n",
    "                continue\n",
    "\n",
    "            acc_results.append(accuracy)\n",
    "            print(f'user_id: {user_id}, video_id: {video_ids[i]}, accuracy: {accuracy}.')\n",
    "\n",
    "        # 若对于某个用户没有任何准确度数据（可能该用户没有对应的数据），则不添加到最终的结果中\n",
    "        if acc_results:\n",
    "            user_acc_results[user_id] = acc_results\n",
    "\n",
    "    return user_acc_results\n",
    "\n",
    "\n",
    "\n",
    "# 计算各个测试集的准确率\n",
    "user_accuracy_results = compute_accuracy_per_user(model, data_with_labels, unique_labels, label_map)\n",
    "\n",
    "print(f'user_accuracy_results: {user_accuracy_results}')\n",
    "\n",
    "# 创建箱形图\n",
    "plt.figure(figsize=(20, 10)) # You might need to adjust the figure size\n",
    "plt.boxplot([user_accuracy_results[user_id] for user_id in sorted(user_accuracy_results.keys())], labels=sorted(user_accuracy_results.keys()))\n",
    "plt.xlabel('User_ID')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for each user')\n",
    "plt.xticks(rotation=90)  # It might be necessary to rotate the x-tick labels for better visualization\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b37ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T01:32:54.029491Z",
     "start_time": "2023-05-20T01:32:53.879330Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reorganize_accuracy_by_video(user_accuracy_results):\n",
    "    video_acc_results = {i: [] for i in range(0, 9)}\n",
    "    for user_id, acc_list in user_accuracy_results.items():\n",
    "        for video_id, accuracy in enumerate(acc_list):\n",
    "            video_acc_results[video_id].append(accuracy)\n",
    "    return video_acc_results\n",
    "\n",
    "video_accuracy_results = reorganize_accuracy_by_video(user_accuracy_results)\n",
    "\n",
    "plt.figure(figsize=(20, 10)) # You might need to adjust the figure size\n",
    "plt.boxplot([video_accuracy_results[video_id] for video_id in sorted(video_accuracy_results.keys())], labels=sorted(video_accuracy_results.keys()))\n",
    "plt.xlabel('Video_ID')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('CaseB Accuracy for each video')\n",
    "plt.xticks(rotation=90)  # It might be necessary to rotate the x-tick labels for better visualization\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d0bc0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T00:28:51.810725Z",
     "start_time": "2023-05-20T00:28:51.751727Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
